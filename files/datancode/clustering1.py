# -*- coding: utf-8 -*-
"""Hard Clustering Final Bismillah.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1odUIxns101-c3WEK4JrEjLOk7f1aXtr7
"""

import pandas as pd

df = pd.read_excel('data_used_1622_revisi_bismillah.xlsx')
#mask = df['kota'] != "Kab. Kepulauan Seribu"
#df_filtered = df[mask]
#df = df_filtered
#df.drop(columns='year', inplace=True)
df.drop(columns='Unnamed: 0', inplace=True)
#df.drop(columns='Kepadatan Penduduk (Jiwa/Km2)', inplace=True)
#df.drop(columns='Jumlah Pekerja (Jiwa)', inplace=True)
#df.drop(columns='Laju Pertumbuhan PDRB (Persen)', inplace=True)
#df.drop(columns='Jumlah Rumah Tangga', inplace=True)
#df.drop(columns='Jumlah Penduduk 15th Bekerja (Jiwa)', inplace=True)


df

df.describe()



"""## K-Means"""

df = pd.read_excel('data_used_1622_revisi_bismillah.xlsx')
#mask = df['kota'] != "Kab. Kepulauan Seribu"
#df_filtered = df[mask]
#df = df_filtered
#df.drop(columns='year', inplace=True)
df.drop(columns='Unnamed: 0', inplace=True)
#df.drop(columns='Kepadatan Penduduk (Jiwa/Km2)', inplace=True)
#df.drop(columns='Jumlah Pekerja (Jiwa)', inplace=True)
#df.drop(columns='Laju Pertumbuhan PDRB (Persen)', inplace=True)
#df.drop(columns='Jumlah Rumah Tangga', inplace=True)
#df.drop(columns='Jumlah Penduduk 15th Bekerja (Jiwa)', inplace=True)


df

# Importing Modules untuk Notebook ini

import warnings; warnings.simplefilter('ignore')
try:
    import google.colab; IN_COLAB = True
    !pip install umap-learn
    !wget https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/tau_unsup.py
except:
    IN_COLAB = False
    print("Running the code locally, please make sure all the python module versions agree with colab environment and all data/assets downloaded")
# Importing Modules untuk Notebook ini

import warnings; warnings.simplefilter('ignore')
import umap, numpy as np, matplotlib.pyplot as plt, pandas as pd, seaborn as sns
from sklearn import cluster
from sklearn.metrics import silhouette_score as siluet
from sklearn.metrics.cluster import homogeneity_score as purity
from sklearn.metrics import normalized_mutual_info_score as NMI
import tau_unsup as tau
import umap.umap_ as umap

sns.set(style="ticks", color_codes=True)
random_state = 99 #jangan lupa random state
sns.set(rc={'figure.figsize':(14,8)})

#pip install umap

from sklearn.cluster import KMeans

X = df.iloc[:,3:]
distorsions, k1, kN = [], 2, 10
for k in range(k1, kN):
    kmeans = cluster.KMeans(n_clusters=k).fit(X)
    distorsions.append(kmeans.inertia_)

plt.plot(range(k1, kN), distorsions, linewidth=10); plt.grid(True)
plt.rcParams.update({'font.size': 18})
plt.title('Elbow curve', fontsize=24)

import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt

# Assuming you have loaded or generated your data as X
X = df.iloc[:, 3:]

# Define a range of cluster numbers to consider
k1, kN = 2, 10
range_n_clusters = range(k1, kN)

silhouette_scores = []

for n_clusters in range_n_clusters:
    kmeans = KMeans(n_clusters=n_clusters)
    cluster_labels = kmeans.fit_predict(X)

    silhouette_avg = silhouette_score(X, cluster_labels)
    silhouette_scores.append(silhouette_avg)

# Plot the Silhouette Score for different cluster numbers
plt.plot(range_n_clusters, silhouette_scores, 'bo-')
plt.xlabel('Number of Clusters')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Score for Different Cluster Numbers')
plt.grid(True)
plt.show()

# Find the best number of clusters that maximizes the Silhouette Score
best_num_clusters = range_n_clusters[np.argmax(silhouette_scores)]
print(f"The best number of clusters is: {best_num_clusters}")

k = 3
km = cluster.KMeans(n_clusters=k, init='random', random_state = random_state)
km.fit(X)
# Hasil clusteringnya
C_km = km.predict(X)
p= sns.countplot(x=C_km)

k = 3
kmPP1 = cluster.KMeans(n_clusters=k, init='k-means++',random_state = random_state)
kmPP1.fit(X)
C_kmpp1 = kmPP1.predict(X)

sns.countplot(x=C_kmpp1)
C_kmpp1[:10]

k = 4
kmPP2 = cluster.KMeans(n_clusters=k, init='k-means++',random_state = random_state)
kmPP2.fit(X)
C_kmpp2 = kmPP2.predict(X)

sns.countplot(x=C_kmpp2)
C_kmpp2[:10]

k = 5
kmPP3 = cluster.KMeans(n_clusters=k, init='k-means++', random_state = random_state)
kmPP3.fit(X)
C_kmpp3 = kmPP3.predict(X)

sns.countplot(x=C_kmpp3)
C_kmpp3[:10]

Hasil_Clustering = [C_km, C_kmpp1, C_kmpp2, C_kmpp3]
for res in Hasil_Clustering:
    print(siluet(X,res), end=', ')

km.inertia_

kmPP1.inertia_

kmPP2.inertia_

kmPP3.inertia_

"""### 3cluster"""

df['k-means++ 3'] = C_kmpp1
#g = sns.pairplot(g1.iloc[:,2:], \
                 #hue="k-means++ 5", diag_kind="hist", palette="tab10")

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# Create a DataFrame for the PCA results
pca_df = pd.DataFrame(data=X_pca, columns=['PCA1', 'PCA2'])
 # Assign cluster labels
pca_df['Cluster'] = C_kmpp1
# Create a scatter plot with colors indicating cluster assignments
plt.figure(figsize=(10, 6))
sns.scatterplot(x='PCA1', y='PCA2', hue='Cluster', data=pca_df, palette='Set1', s=100)
plt.title('PCA Visualization of K-Means Clusters')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend(title='Cluster')
plt.show()

df

df['k-means++ 3'].value_counts()

all0 = df.loc[df['k-means++ 3']==0]
all1 = df.loc[df['k-means++ 3']==1]
all2 = df.loc[df['k-means++ 3']==2]

all0

all0.mean()

all0[['provinsi','kota']]

all0['kota'].unique()

all1

all1.mean()

all1['kota'].unique()

all2

all2.mean()

all2['kota'].unique()

"""### 4cluster

"""

df['k-means++ 4'] = C_kmpp2
#g = sns.pairplot(g1.iloc[:,2:], \
                 #hue="k-means++ 5", diag_kind="hist", palette="tab10")

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# Create a DataFrame for the PCA results
pca_df = pd.DataFrame(data=X_pca, columns=['PCA1', 'PCA2'])
 # Assign cluster labels
pca_df['Cluster'] = C_kmpp2
# Create a scatter plot with colors indicating cluster assignments
plt.figure(figsize=(10, 6))
sns.scatterplot(x='PCA1', y='PCA2', hue='Cluster', data=pca_df, palette='Set1', s=100)
plt.title('PCA Visualization of K-Means Clusters')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend(title='Cluster')
plt.show()

df

df['k-means++ 4'].value_counts()

all0 = df.loc[df['k-means++ 4']==0]
all1 = df.loc[df['k-means++ 4']==1]
all2 = df.loc[df['k-means++ 4']==2]

all0

all0.mean()

all0[['provinsi','kota']]

all0['kota'].unique()

all1

all1.mean()

all1['kota'].unique()

all2

all2.mean()

all2['kota'].unique()

all3 = df.loc[df['k-means++ 4']==3]
all3

all3.mean()

all3['kota'].unique()



"""### 5cluster

"""

df['k-means++ 5'] = C_kmpp3
#g = sns.pairplot(g1.iloc[:,2:], \
                 #hue="k-means++ 5", diag_kind="hist", palette="tab10")

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# Create a DataFrame for the PCA results
pca_df = pd.DataFrame(data=X_pca, columns=['PCA1', 'PCA2'])
 # Assign cluster labels
pca_df['Cluster'] = C_kmpp3
# Create a scatter plot with colors indicating cluster assignments
plt.figure(figsize=(10, 6))
sns.scatterplot(x='PCA1', y='PCA2', hue='Cluster', data=pca_df, palette='Set1', s=100)
plt.title('PCA Visualization of K-Means Clusters')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend(title='Cluster')
plt.show()

df

df['k-means++ 5'].value_counts()

all0 = df.loc[df['k-means++ 5']==0]
all1 = df.loc[df['k-means++ 5']==1]
all2 = df.loc[df['k-means++ 5']==2]

all0

all0.mean()

all0[['provinsi','kota']]

all0['kota'].unique()

all1

all1.mean()

all1['kota'].unique()

all2

all2.mean()

all2['kota'].unique()

all3 = df.loc[df['k-means++ 5']==3]
all3

all3.mean()

all3['kota'].unique()

all4 = df.loc[df['k-means++ 5']==4]
all4

all4.mean()

all4['kota'].unique()



"""## Kmedoids"""

import pandas as pd

df = pd.read_excel('data_used_1622_revisi_bismillah.xlsx')
#mask = df['kota'] != "Kab. Kepulauan Seribu"
#df_filtered = df[mask]
#df = df_filtered
#df.drop(columns='year', inplace=True)
df.drop(columns='Unnamed: 0', inplace=True)
#df.drop(columns='Kepadatan Penduduk (Jiwa/Km2)', inplace=True)
#df.drop(columns='Jumlah Pekerja (Jiwa)', inplace=True)
#df.drop(columns='Laju Pertumbuhan PDRB (Persen)', inplace=True)
#df.drop(columns='Jumlah Rumah Tangga', inplace=True)
#df.drop(columns='Jumlah Penduduk 15th Bekerja (Jiwa)', inplace=True)


df

X = df.iloc[:,3:]
X
random_state = 99 #jangan lupa random state

pip install pyclustering

"""### 3cluster"""

import pandas as pd

df = pd.read_excel('data_used_1622_revisi_bismillah.xlsx')
#mask = df['kota'] != "Kab. Kepulauan Seribu"
#df_filtered = df[mask]
#df = df_filtered
#df.drop(columns='year', inplace=True)
df.drop(columns='Unnamed: 0', inplace=True)
#df.drop(columns='Kepadatan Penduduk (Jiwa/Km2)', inplace=True)
#df.drop(columns='Jumlah Pekerja (Jiwa)', inplace=True)
#df.drop(columns='Laju Pertumbuhan PDRB (Persen)', inplace=True)
#df.drop(columns='Jumlah Rumah Tangga', inplace=True)
#df.drop(columns='Jumlah Penduduk 15th Bekerja (Jiwa)', inplace=True)


df

X = df.iloc[:,3:]
X

import numpy as np
from pyclustering.cluster.kmedoids import kmedoids
from pyclustering.cluster import cluster_visualizer
import matplotlib.pyplot as plt

# Specify the number of clusters (K)
k = 3

# Create a KMedoids instance
X_np = X.to_numpy()  # Convert DataFrame to NumPy array
kmedoids_instance = kmedoids(X_np, initial_index_medoids=np.random.choice(len(X), k, replace=False))

# Run the K-Medoids algorithm
kmedoids_instance.process()
clusters = kmedoids_instance.get_clusters()
medoids = kmedoids_instance.get_medoids()

df['Cluster'] = None

# Print the cluster labels
for i, cluster in enumerate(clusters):
    df.loc[cluster, 'Cluster'] = i + 1

df

import numpy as np
import pandas as pd
from pyclustering.cluster.kmedoids import kmedoids
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# Assuming you have already defined k, kmedoids_instance, clusters, medoids, and df as in your code.

# Perform PCA to reduce the dimensionality
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_np)

# Add PCA components to the DataFrame
df['PCA1'] = X_pca[:, 0]
df['PCA2'] = X_pca[:, 1]

# Create a scatter plot with colors indicating cluster assignments
plt.figure(figsize=(10, 6))
for i, cluster in enumerate(clusters):
    plt.scatter(df.loc[cluster, 'PCA1'], df.loc[cluster, 'PCA2'], label=f'Cluster {i + 1}')

# Plot medoids as black stars
medoid_indices = medoids
medoid_coordinates = X_pca[medoid_indices]
plt.scatter(medoid_coordinates[:, 0], medoid_coordinates[:, 1], marker='*', s=100, c='black', label='Medoids')

plt.title('PCA Visualization of K-Medoids Clusters')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend()
plt.grid()
plt.show()

df

df['Cluster'].value_counts()

all0 = df.loc[df['Cluster']==1]
all1 = df.loc[df['Cluster']==2]
all2 = df.loc[df['Cluster']==3]

all0

all0.mean()

all0[['provinsi','kota']]

all0['kota'].unique()

all1

all1.mean()

all1['kota'].unique()

all2

all2.mean()

all2['kota'].unique()



"""### 4cluster

"""

import pandas as pd

df = pd.read_excel('data_used_1622_revisi_bismillah.xlsx')
#mask = df['kota'] != "Kab. Kepulauan Seribu"
#df_filtered = df[mask]
#df = df_filtered
#df.drop(columns='year', inplace=True)
df.drop(columns='Unnamed: 0', inplace=True)
#df.drop(columns='Kepadatan Penduduk (Jiwa/Km2)', inplace=True)
#df.drop(columns='Jumlah Pekerja (Jiwa)', inplace=True)
#df.drop(columns='Laju Pertumbuhan PDRB (Persen)', inplace=True)
#df.drop(columns='Jumlah Rumah Tangga', inplace=True)
#df.drop(columns='Jumlah Penduduk 15th Bekerja (Jiwa)', inplace=True)


df

X = df.iloc[:,3:]
X

import numpy as np
from pyclustering.cluster.kmedoids import kmedoids
from pyclustering.cluster import cluster_visualizer
import matplotlib.pyplot as plt

# Specify the number of clusters (K)
k = 4

# Create a KMedoids instance
X_np = X.to_numpy()  # Convert DataFrame to NumPy array
kmedoids_instance = kmedoids(X_np, initial_index_medoids=np.random.choice(len(X), k, replace=False))

# Run the K-Medoids algorithm
kmedoids_instance.process()
clusters = kmedoids_instance.get_clusters()
medoids = kmedoids_instance.get_medoids()

df['Cluster'] = None

# Print the cluster labels
for i, cluster in enumerate(clusters):
    df.loc[cluster, 'Cluster'] = i + 1

df

import numpy as np
import pandas as pd
from pyclustering.cluster.kmedoids import kmedoids
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# Assuming you have already defined k, kmedoids_instance, clusters, medoids, and df as in your code.

# Perform PCA to reduce the dimensionality
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_np)

# Add PCA components to the DataFrame
df['PCA1'] = X_pca[:, 0]
df['PCA2'] = X_pca[:, 1]

# Create a scatter plot with colors indicating cluster assignments
plt.figure(figsize=(10, 6))
for i, cluster in enumerate(clusters):
    plt.scatter(df.loc[cluster, 'PCA1'], df.loc[cluster, 'PCA2'], label=f'Cluster {i + 1}')

# Plot medoids as black stars
medoid_indices = medoids
medoid_coordinates = X_pca[medoid_indices]
plt.scatter(medoid_coordinates[:, 0], medoid_coordinates[:, 1], marker='*', s=100, c='black', label='Medoids')

plt.title('PCA Visualization of K-Medoids Clusters')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend()
plt.grid()
plt.show()

df['Cluster'].value_counts()

all0 = df.loc[df['Cluster']==0]
all1 = df.loc[df['Cluster']==1]
all2 = df.loc[df['Cluster']==2]

all0

all0.mean()

all0[['provinsi','kota']]

all0['kota'].unique()

all1

all1.mean()

all1['kota'].unique()

all2

all2.mean()

all2['kota'].unique()

all3 = df.loc[df['Cluster']==3]
all3

all3.mean()

all3['kota'].unique()



"""### 5cluster

"""

import pandas as pd

df = pd.read_excel('data_used_1622_revisi_bismillah.xlsx')
#mask = df['kota'] != "Kab. Kepulauan Seribu"
#df_filtered = df[mask]
#df = df_filtered
#df.drop(columns='year', inplace=True)
df.drop(columns='Unnamed: 0', inplace=True)
#df.drop(columns='Kepadatan Penduduk (Jiwa/Km2)', inplace=True)
#df.drop(columns='Jumlah Pekerja (Jiwa)', inplace=True)
#df.drop(columns='Laju Pertumbuhan PDRB (Persen)', inplace=True)
#df.drop(columns='Jumlah Rumah Tangga', inplace=True)
#df.drop(columns='Jumlah Penduduk 15th Bekerja (Jiwa)', inplace=True)


df

X = df.iloc[:,3:]
X

import numpy as np
from pyclustering.cluster.kmedoids import kmedoids
from pyclustering.cluster import cluster_visualizer
import matplotlib.pyplot as plt

# Specify the number of clusters (K)
k = 5

# Create a KMedoids instance
X_np = X.to_numpy()  # Convert DataFrame to NumPy array
kmedoids_instance = kmedoids(X_np, initial_index_medoids=np.random.choice(len(X), k, replace=False))

# Run the K-Medoids algorithm
kmedoids_instance.process()
clusters = kmedoids_instance.get_clusters()
medoids = kmedoids_instance.get_medoids()

df['Cluster'] = None

# Print the cluster labels
for i, cluster in enumerate(clusters):
    df.loc[cluster, 'Cluster'] = i + 1

df

import numpy as np
import pandas as pd
from pyclustering.cluster.kmedoids import kmedoids
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# Assuming you have already defined k, kmedoids_instance, clusters, medoids, and df as in your code.

# Perform PCA to reduce the dimensionality
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_np)

# Add PCA components to the DataFrame
df['PCA1'] = X_pca[:, 0]
df['PCA2'] = X_pca[:, 1]

# Create a scatter plot with colors indicating cluster assignments
plt.figure(figsize=(10, 6))
for i, cluster in enumerate(clusters):
    plt.scatter(df.loc[cluster, 'PCA1'], df.loc[cluster, 'PCA2'], label=f'Cluster {i + 1}')

# Plot medoids as black stars
medoid_indices = medoids
medoid_coordinates = X_pca[medoid_indices]
plt.scatter(medoid_coordinates[:, 0], medoid_coordinates[:, 1], marker='*', s=100, c='black', label='Medoids')

plt.title('PCA Visualization of K-Medoids Clusters')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend()
plt.grid()
plt.show()

df['Cluster'].value_counts()

all0 = df.loc[df['Cluster']==0]
all1 = df.loc[df['Cluster']==1]
all2 = df.loc[df['Cluster']==2]

all0

all0.mean()

all0[['provinsi','kota']]

all0['kota'].unique()

all1

all1.mean()

all1['kota'].unique()

all2

all2.mean()

all2['kota'].unique()

all3 = df.loc[df['Cluster']==3]
all3

all3.mean()

all3['kota'].unique()

all4 = df.loc[df['Cluster']==4]
all4

all4.mean()

all4['kota'].unique()

"""# Evaluation

## Dunn Index
"""

import pandas as pd

df = pd.read_excel('data_used_1622_revisi_bismillah.xlsx')
#mask = df['kota'] != "Kab. Kepulauan Seribu"
#df_filtered = df[mask]
#df = df_filtered
#df.drop(columns='year', inplace=True)
df.drop(columns='Unnamed: 0', inplace=True)
#df.drop(columns='Kepadatan Penduduk (Jiwa/Km2)', inplace=True)
#df.drop(columns='Jumlah Pekerja (Jiwa)', inplace=True)
#df.drop(columns='Laju Pertumbuhan PDRB (Persen)', inplace=True)
#df.drop(columns='Jumlah Rumah Tangga', inplace=True)
#df.drop(columns='Jumlah Penduduk 15th Bekerja (Jiwa)', inplace=True)


df
X = df.iloc[:,3:]
X
random_state = 99 #jangan lupa random state

X.info()

import numpy as np
import pandas as pd
from sklearn import cluster
from pyclustering.cluster.kmedoids import kmedoids
from sklearn.metrics import pairwise_distances
from scipy.spatial import distance
from itertools import combinations

X_np = X.to_numpy()  # Convert DataFrame to NumPy array
# Define a function to calculate the Dunn Index
def dunn_index(clusters, distance_matrix):
    max_intracluster_distances = []
    min_intercluster_distances = []

    # Calculate maximum intra-cluster distances
    for cluster in clusters:
        distances = distance_matrix[np.ix_(cluster, cluster)]
        max_intracluster_distances.append(np.max(distances))

    # Calculate minimum inter-cluster distances
    cluster_combinations = list(combinations(clusters, 2))
    for combo in cluster_combinations:
        distances = distance_matrix[np.ix_(combo[0], combo[1])]
        min_intercluster_distances.append(np.min(distances))

    # Calculate Dunn Index
    dunn_index = min(min_intercluster_distances) / max(max_intracluster_distances)
    return dunn_index

# Create instances of clustering algorithms
clustering_algorithms = {
    "K-Means ++ 3": cluster.KMeans(n_clusters=3, init='k-means++',random_state = random_state),
    "K-Means ++ 4": cluster.KMeans(n_clusters=4, init='k-means++',random_state = random_state),
    "K-Means ++ 5": cluster.KMeans(n_clusters=5, init='k-means++',random_state = random_state),

}

# Calculate Dunn Index for each clustering algorithm
dunn_scores = {}
for name, algorithm in clustering_algorithms.items():
    algorithm.fit(X)
    labels = algorithm.labels_
    clusters = [np.where(labels == i)[0] for i in range(max(labels) + 1)]
    distance_matrix = pairwise_distances(X)
    dunn = dunn_index(clusters, distance_matrix)
    dunn_scores[name] = dunn

# Print Dunn Index scores for comparison
for name, dunn in dunn_scores.items():
    print(f"{name}: Dunn Index = {dunn:.4f}")

import numpy as np
import pandas as pd
from sklearn import cluster
from pyclustering.cluster.kmedoids import kmedoids
from sklearn.metrics import pairwise_distances
from scipy.spatial import distance
from itertools import combinations

X_np = X.to_numpy()  # Convert DataFrame to NumPy array

# Define a function to calculate the Dunn Index
def dunn_index(clusters, distance_matrix):
    max_intracluster_distances = []
    min_intercluster_distances = []

    # Calculate maximum intra-cluster distances
    for cluster in clusters:
        distances = distance_matrix[np.ix_(cluster, cluster)]
        max_intracluster_distances.append(np.max(distances))

    # Calculate minimum inter-cluster distances
    cluster_combinations = list(combinations(clusters, 2))
    for combo in cluster_combinations:
        distances = distance_matrix[np.ix_(combo[0], combo[1])]
        min_intercluster_distances.append(np.min(distances))

    # Calculate Dunn Index
    dunn_index = min(min_intercluster_distances) / max(max_intracluster_distances)
    return dunn_index

# Create instances of clustering algorithms
clustering_algorithms = {
    "K-Medoids 3": kmedoids(X_np, initial_index_medoids=np.random.choice(len(X), 3, replace=False)),
    "K-Medoids 4": kmedoids(X_np, initial_index_medoids=np.random.choice(len(X), 4, replace=False)),
    "K-Medoids 5": kmedoids(X_np, initial_index_medoids=np.random.choice(len(X), 5, replace=False))
}

# Calculate Dunn Index for each clustering algorithm
dunn_scores = {}
for name, algorithm in clustering_algorithms.items():
    algorithm.process()
    labels = algorithm.get_clusters()
    distance_matrix = pairwise_distances(X)
    dunn = dunn_index(labels, distance_matrix)
    dunn_scores[name] = dunn

# Print Dunn Index scores for comparison
for name, dunn in dunn_scores.items():
    print(f"{name}: Dunn Index = {dunn:.4f}")

"""## Silhouette Score"""

import numpy as np
import pandas as pd
from sklearn import cluster
from sklearn.metrics import silhouette_score

# Assuming you have loaded or generated your data as X
# Create instances of clustering algorithms
clustering_algorithms = {
    "K-Means ++ 3": cluster.KMeans(n_clusters=3, init='k-means++', random_state=random_state),
    "K-Means ++ 4": cluster.KMeans(n_clusters=4, init='k-means++', random_state=random_state),
    "K-Means ++ 5": cluster.KMeans(n_clusters=5, init='k-means++', random_state=random_state)
}

# Calculate Silhouette Score for each clustering algorithm
silhouette_scores = {}
for name, algorithm in clustering_algorithms.items():
    algorithm.fit(X)  # Fit the algorithm to your data
    labels = algorithm.labels_
    silhouette = silhouette_score(X, labels)
    silhouette_scores[name] = silhouette

# Print Silhouette Scores for comparison
for name, silhouette in silhouette_scores.items():
    print(f"{name}: Silhouette Score = {silhouette:.4f}")

import numpy as np
import pandas as pd
from sklearn import cluster
from pyclustering.cluster.kmedoids import kmedoids
from sklearn.metrics import silhouette_score
from itertools import combinations

X_np = X.to_numpy()  # Convert DataFrame to NumPy array

# Create instances of clustering algorithms
clustering_algorithms = {
    "K-Medoids 3": kmedoids(X_np, initial_index_medoids=np.random.choice(len(X), 3, replace=False)),
    "K-Medoids 4": kmedoids(X_np, initial_index_medoids=np.random.choice(len(X), 4, replace=False)),
    "K-Medoids 5": kmedoids(X_np, initial_index_medoids=np.random.choice(len(X), 5, replace=False))
}

# Calculate Silhouette Score for each clustering algorithm
silhouette_scores = {}
for name, algorithm in clustering_algorithms.items():
    algorithm.process()
    labels = algorithm.get_clusters()
    labels_flat = [i for i, cluster in enumerate(labels) for _ in cluster]  # Flatten labels
    silhouette = silhouette_score(X_np, labels_flat)
    silhouette_scores[name] = silhouette

# Print Silhouette Scores for comparison
for name, silhouette in silhouette_scores.items():
    print(f"{name}: Silhouette Score = {silhouette:.4f}")

"""## Davies Bouldin"""

import numpy as np
import pandas as pd
from sklearn import cluster
from sklearn.metrics import davies_bouldin_score

# Assuming you have loaded or generated your data as X
# Create instances of clustering algorithms
clustering_algorithms = {
    "K-Means ++ 3": cluster.KMeans(n_clusters=3, init='k-means++', random_state=random_state),
    "K-Means ++ 4": cluster.KMeans(n_clusters=4, init='k-means++', random_state=random_state),
    "K-Means ++ 5": cluster.KMeans(n_clusters=5, init='k-means++', random_state=random_state)
}

# Calculate Davies-Bouldin Index for each clustering algorithm
dbi_scores = {}
for name, algorithm in clustering_algorithms.items():
    algorithm.fit(X)  # Fit the algorithm to your data
    labels = algorithm.labels_
    dbi = davies_bouldin_score(X, labels)
    dbi_scores[name] = dbi

# Print Davies-Bouldin Index scores for comparison
for name, dbi in dbi_scores.items():
    print(f"{name}: Davies-Bouldin Index = {dbi:.4f}")

import numpy as np
import pandas as pd
from pyclustering.cluster.kmedoids import kmedoids
from sklearn.metrics import davies_bouldin_score

# Assuming you have loaded or generated your data as X
X_np = X.to_numpy()
# Create instances of clustering algorithms
clustering_algorithms = {
    "K-Medoids 3": kmedoids(X_np, initial_index_medoids=np.random.choice(len(X), 3, replace=False)),
    "K-Medoids 4": kmedoids(X_np, initial_index_medoids=np.random.choice(len(X), 4, replace=False)),
    "K-Medoids 5": kmedoids(X_np, initial_index_medoids=np.random.choice(len(X), 5, replace=False))
}

# Calculate Davies-Bouldin Index for each clustering algorithm
dbi_scores = {}
for name, algorithm in clustering_algorithms.items():
    algorithm.process()
    clusters = algorithm.get_clusters()

    # Convert clusters to a format compatible with the metric
    cluster_labels = np.zeros(len(X))
    for i, cluster in enumerate(clusters):
        cluster_labels[cluster] = i

    dbi = davies_bouldin_score(X, cluster_labels)
    dbi_scores[name] = dbi

# Print Davies-Bouldin Index scores for comparison
for name, dbi in dbi_scores.items():
    print(f"{name}: Davies-Bouldin Index = {dbi:.4f}")

"""## Calinski-Harabasz index

"""

import numpy as np
import pandas as pd
from sklearn import cluster
from sklearn import metrics

# Assuming you have loaded or generated your data as X
# Create instances of clustering algorithms
clustering_algorithms = {
    "K-Means ++ 3": cluster.KMeans(n_clusters=3, init='k-means++', random_state=random_state),
    "K-Means ++ 4": cluster.KMeans(n_clusters=4, init='k-means++', random_state=random_state),
    "K-Means ++ 5": cluster.KMeans(n_clusters=5, init='k-means++', random_state=random_state)
}

# Calculate Calinski-Harabasz Index for each clustering algorithm
ch_scores = {}
for name, algorithm in clustering_algorithms.items():
    algorithm.fit(X)  # Fit the algorithm to your data
    labels = algorithm.labels_
    ch = metrics.calinski_harabasz_score(X, labels)
    ch_scores[name] = ch

# Print Calinski-Harabasz Index scores for comparison
for name, ch in ch_scores.items():
    print(f"{name}: Calinski-Harabasz Index = {ch:.4f}")

import numpy as np
from pyclustering.cluster.kmedoids import kmedoids
from sklearn.metrics import pairwise_distances
from itertools import combinations
from sklearn.metrics import calinski_harabasz_score

X_np = X.to_numpy()  # Convert DataFrame to NumPy array

# Create instances of clustering algorithms
clustering_algorithms = {
    "K-Medoids 3": kmedoids(X_np, initial_index_medoids=np.random.choice(len(X), 3, replace=False)),
    "K-Medoids 4": kmedoids(X_np, initial_index_medoids=np.random.choice(len(X), 4, replace=False)),
    "K-Medoids 5": kmedoids(X_np, initial_index_medoids=np.random.choice(len(X), 5, replace=False))
}

# Calculate Calinski-Harabasz Index for each clustering algorithm
calinski_harabasz_scores = {}
for name, algorithm in clustering_algorithms.items():
    algorithm.process()
    clusters = algorithm.get_clusters()

    # Convert clusters to a format compatible with the metric
    cluster_labels = np.zeros(len(X))
    for i, cluster in enumerate(clusters):
        cluster_labels[cluster] = i

    ch_score = calinski_harabasz_score(X_np, cluster_labels)
    calinski_harabasz_scores[name] = ch_score

# Print Calinski-Harabasz Index scores for comparison
for name, ch_score in calinski_harabasz_scores.items():
    print(f"{name}: Calinski-Harabasz Index = {ch_score:.4f}")

